{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "f2fb1202-2e23-43fc-8fdb-c362611d67b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4a2390-0b27-417a-8a21-aac69556a077",
   "metadata": {},
   "source": [
    "## Load IRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "57bb6f93-31d2-4456-a44a-e64600b27143",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['Unnamed: 0',\n",
    "'unit.IRI_KEY',\n",
    "'unit.SY',\n",
    "'unit.GE',\n",
    "'unit.VEND',\n",
    "'unit.ITEM',\n",
    "'price.IRI_KEY',\n",
    "'price.SY',\n",
    "'price.GE',\n",
    "'price.VEND',\n",
    "'price.ITEM',\n",
    "'price.cate',\n",
    "'F.IRI_KEY',\n",
    "'F.SY',\n",
    "'F.GE',\n",
    "'F.VEND',\n",
    "'F.ITEM',\n",
    "'F.cate',\n",
    "'D.IRI_KEY',\n",
    "'D.SY',\n",
    "'D.GE',\n",
    "'D.VEND',\n",
    "'D.ITEM',\n",
    "'D.cate',\n",
    "'holiday.IRI_KEY',\n",
    "'holiday.SY',\n",
    "'holiday.GE',\n",
    "'holiday.VEND',\n",
    "'holiday.ITEM',\n",
    "'holiday.cate',]\n",
    "\n",
    "def read_csv(i):\n",
    "    # read each slots csv, create tensor of dimension:\n",
    "    # (num_timeseries, lenght, features)\n",
    "    dataset = []\n",
    "    data = pd.read_csv(f\"dataset/iri{i}.csv\", usecols=lambda x: x not in drop_columns)\n",
    "\n",
    "    cl = data.columns.to_list()\n",
    "    clcat = ['unit.cate'] \n",
    "    clu = [c for c in cl if 'unit.1' in c]\n",
    "    clp = [c for c in cl if 'price.1' in c]\n",
    "    clh = [c for c in cl if 'holiday.1' in c]\n",
    "    clf = [c for c in cl if 'F.' in c]\n",
    "    cld = [c for c in cl if 'D.' in c]\n",
    "\n",
    "    \n",
    "    dataset.append(data[clu].values)\n",
    "    #replace some inf values in price\n",
    "    data_clp = data[clp].replace(np.inf, np.nan).interpolate()\n",
    "    dataset.append(data_clp.values)\n",
    "    dataset.append(data[clh].values)\n",
    "    dataset.append(data[clf].values)\n",
    "    dataset.append(data[cld].values)\n",
    "    dataset = np.array(dataset)\n",
    "    dataset = np.transpose(dataset,(1,2,0))\n",
    "    return dataset , data[clcat].values\n",
    "\n",
    "def normalize(d):\n",
    "    # normalize unit and price for NNs\n",
    "    norm = torch.nn.InstanceNorm1d(2)\n",
    "    dd_norm = norm(d[:,:,:2])\n",
    "    d[:,:,:2] = dd_norm\n",
    "    return d\n",
    "\n",
    "def concat_slots(fist_slot, last_slot):\n",
    "    # concat slots to create whole train/valid/test dataset \n",
    "    dataset = []\n",
    "    catg = []\n",
    "    for i in range(fist_slot, last_slot):\n",
    "        tens , cat = read_csv(i)\n",
    "        dataset.append(tens)\n",
    "        catg.append(cat)\n",
    "\n",
    "    catg = np.concatenate(catg,axis=0)\n",
    "    dataset = np.concatenate(dataset,axis=0)\n",
    "    return dataset , catg #np.array , torch.array, np.array\n",
    "\n",
    "def mode_indx(mode):\n",
    "    if mode == \"train\":\n",
    "        (start,end) = (1,8) #(60993, 55, 5)\n",
    "    if mode == \"valid\":\n",
    "        (start,end) = (8,11)#(22951, 55, 5)\n",
    "    if mode == \"test\":     \n",
    "        (start,end) = (11,16)#(36194, 55, 5)\n",
    "    return (start,end)\n",
    "\n",
    "def get_main_data(mode):\n",
    "    start, end = mode_indx(mode)\n",
    "    dataset, _ = concat_slots(start, end)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_data_slot(slot):\n",
    "    dataset, _ = concat_slots(slot, slot+1)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_base_forecasts(horizon, mode):\n",
    "    base_forecasters = np.load(f'base_forecasters/{horizon}/_all_npy/base_{mode}.npy')\n",
    "    return base_forecasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "f57a444f-4a4f-4ad2-a5f3-1eed21737c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3cd311-b1ff-4694-be8f-0cd2e72a6a09",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "c0800a7b-e9ab-4635-a7a7-662f4c192e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'FFORMA1'\n",
    "horizon = 7\n",
    "\n",
    "if model == 'FFORMA2':\n",
    "    use_influentional = True\n",
    "elif model == 'FFORMA1':\n",
    "    use_influentional = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "da1ea3c3-9d5f-42ee-aca1-f6670a49b876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing slot 1...\n",
      "exctacting features of dim  0 ...\n",
      "exctacting features of dim  1 ...\n",
      "exctacting features of dim  2 ...\n",
      "exctacting features of dim  3 ...\n",
      "exctacting features of dim  4 ...\n",
      "processing slot 2...\n",
      "exctacting features of dim  0 ...\n",
      "exctacting features of dim  1 ...\n",
      "exctacting features of dim  2 ...\n",
      "exctacting features of dim  3 ...\n",
      "exctacting features of dim  4 ...\n",
      "processing slot 3...\n",
      "exctacting features of dim  0 ...\n",
      "exctacting features of dim  1 ...\n",
      "exctacting features of dim  2 ...\n",
      "exctacting features of dim  3 ...\n",
      "exctacting features of dim  4 ...\n",
      "processing slot 4...\n",
      "exctacting features of dim  0 ...\n",
      "exctacting features of dim  1 ...\n",
      "exctacting features of dim  2 ...\n",
      "exctacting features of dim  3 ...\n",
      "exctacting features of dim  4 ...\n",
      "processing slot 5...\n",
      "exctacting features of dim  0 ...\n",
      "exctacting features of dim  1 ...\n",
      "exctacting features of dim  2 ...\n",
      "exctacting features of dim  3 ...\n",
      "exctacting features of dim  4 ...\n",
      "processing slot 6...\n",
      "exctacting features of dim  0 ...\n",
      "exctacting features of dim  1 ...\n",
      "exctacting features of dim  2 ...\n",
      "exctacting features of dim  3 ...\n",
      "exctacting features of dim  4 ...\n",
      "processing slot 7...\n",
      "exctacting features of dim  0 ...\n",
      "exctacting features of dim  1 ...\n",
      "exctacting features of dim  2 ...\n",
      "exctacting features of dim  3 ...\n",
      "exctacting features of dim  4 ...\n",
      "processing slot 8...\n",
      "exctacting features of dim  0 ...\n",
      "exctacting features of dim  1 ...\n",
      "exctacting features of dim  2 ...\n",
      "exctacting features of dim  3 ...\n",
      "exctacting features of dim  4 ...\n",
      "processing slot 9...\n",
      "exctacting features of dim  0 ...\n",
      "exctacting features of dim  1 ...\n",
      "exctacting features of dim  2 ...\n",
      "exctacting features of dim  3 ...\n",
      "exctacting features of dim  4 ...\n",
      "processing slot 10...\n",
      "exctacting features of dim  0 ...\n",
      "exctacting features of dim  1 ...\n",
      "exctacting features of dim  2 ...\n",
      "exctacting features of dim  3 ...\n",
      "exctacting features of dim  4 ...\n",
      "processing slot 11...\n",
      "exctacting features of dim  0 ...\n",
      "exctacting features of dim  1 ...\n",
      "exctacting features of dim  2 ...\n",
      "exctacting features of dim  3 ...\n",
      "exctacting features of dim  4 ...\n",
      "processing slot 12...\n",
      "exctacting features of dim  0 ...\n",
      "exctacting features of dim  1 ...\n",
      "exctacting features of dim  2 ...\n",
      "exctacting features of dim  3 ...\n",
      "exctacting features of dim  4 ...\n",
      "processing slot 13...\n",
      "exctacting features of dim  0 ...\n",
      "exctacting features of dim  1 ...\n",
      "exctacting features of dim  2 ...\n",
      "exctacting features of dim  3 ...\n",
      "exctacting features of dim  4 ...\n",
      "processing slot 14...\n",
      "exctacting features of dim  0 ...\n",
      "exctacting features of dim  1 ...\n",
      "exctacting features of dim  2 ...\n",
      "exctacting features of dim  3 ...\n",
      "exctacting features of dim  4 ...\n",
      "processing slot 15...\n",
      "exctacting features of dim  0 ...\n",
      "exctacting features of dim  1 ...\n",
      "exctacting features of dim  2 ...\n",
      "exctacting features of dim  3 ...\n",
      "exctacting features of dim  4 ...\n"
     ]
    }
   ],
   "source": [
    "from tsfeatures import tsfeatures\n",
    "from tsfeatures import acf_features, arch_stat, crossing_points, entropy, flat_spots, holt_parameters, hurst, lumpiness, nonlinearity, pacf_features, stl_features, stability, unitroot_kpss, unitroot_pp\n",
    "\n",
    "def reshape_for_tsfeatures(Y, dim):\n",
    "    \"\"\"\n",
    "    Reshape the timeseries for tsfeatures\n",
    "    inputs:\n",
    "        Y: the timeseries (numpy array) (num_series x series_length x covariates_dim)\n",
    "    \"\"\"\n",
    "    reshaped = []\n",
    "    for id in range(Y.shape[0]):\n",
    "        for time in range(Y.shape[1]):\n",
    "            row = np.array([id, time])\n",
    "            row = np.hstack([row, Y[id, time, dim]])\n",
    "            reshaped.append(row)\n",
    "    reshaped = np.vstack(reshaped)\n",
    "    return pd.DataFrame.from_records(reshaped, columns=['unique_id', 'ds', 'y'])\n",
    "\n",
    "\n",
    "for slot in range(1, 16):\n",
    "    print(f\"processing slot {slot}...\")\n",
    "    data = get_data_slot(slot)\n",
    "    for dim in range(5):\n",
    "        print('exctacting features of dim ', dim, '...')\n",
    "        if dim==0:\n",
    "            slice = data[:, :55-horizon, :]\n",
    "        else:\n",
    "            slice = data[:, :, :]\n",
    "        \n",
    "        panel = reshape_for_tsfeatures(slice, dim)\n",
    "        if dim == 0:\n",
    "            features = tsfeatures(panel, freq=4, threads=16, scale=False, features=[acf_features, arch_stat, crossing_points, entropy, flat_spots, holt_parameters, hurst, lumpiness, nonlinearity, pacf_features, stl_features, stability, unitroot_kpss, unitroot_pp])\n",
    "        else:\n",
    "            features = tsfeatures(panel, freq=4, threads=16, scale=False, features=[crossing_points, entropy, flat_spots, hurst, lumpiness, stability])\n",
    "        \n",
    "        features.to_csv(f'features/slot_{slot}_h{horizon}_dim{dim}.csv', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "9d9666a4-3894-44c3-9ce2-183ed22b5b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_main_data = get_main_data(\"train\")\n",
    "valid_main_data = get_main_data(\"valid\")\n",
    "test_main_data = get_main_data(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "4574f24a-4b6b-45d5-a29d-f27125170501",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_base_data = get_base_forecasts(horizon, \"train\")\n",
    "valid_base_data = get_base_forecasts(horizon, \"valid\")\n",
    "test_base_data = get_base_forecasts(horizon, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "cde32294-d797-489a-8c57-26fd6a2a6d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_true, y_pred, loss):\n",
    "    \"\"\"\n",
    "    Loss function for the meta-learner\n",
    "    Args:\n",
    "        y_true: true values (numpy array) (num_series x horizon)\n",
    "        Y_pred: the matrix of predictions (num_series x num_models x horizon)\n",
    "    \"\"\"\n",
    "    if loss == 'mse':\n",
    "        return np.mean(np.square(y_true - y_pred), axis=2)\n",
    "    elif loss == 'rmse':\n",
    "        return np.sqrt(np.mean(np.square(y_true - y_pred), axis=2))\n",
    "    elif loss == 'mae':\n",
    "        return np.mean(np.abs(y_true - y_pred), axis=2)\n",
    "    elif callable(loss):\n",
    "        return loss(y_true, y_pred)\n",
    "    else:\n",
    "        raise ValueError('Loss function not supported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "ac7b7591-113c-45eb-8383-f6656027877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_selection(Y_pred, y_true, loss, return_one_hot=False):\n",
    "    \"\"\"\n",
    "    Generate labels for the meta-learner using selection\n",
    "    Args:\n",
    "        y_true: true values (numpy array) (num_series x horizon)\n",
    "        Y_pred: the matrix of predictions (num_series x num_models x horizon)\n",
    "    \"\"\"\n",
    "    error = loss_fn(y_true, np.transpose(Y_pred, (1, 0, 2)), loss)\n",
    "    error = error.T\n",
    "    label_indices = np.argmin(error, axis=1)\n",
    "    if return_one_hot:    \n",
    "        # convert to one-hot encoding\n",
    "        labels = np.zeros(Y_pred.shape[1:])\n",
    "        for i in range(Y_pred.shape[1]):\n",
    "            labels[i, label_indices[i]] = 1.0\n",
    "        return labels\n",
    "    else:\n",
    "        return label_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "634b890c-43f4-453c-b406-60f5d2d866e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = labels_selection(train_base_data, train_main_data[:, -horizon:, 0], 'rmse')\n",
    "valid_Y = labels_selection(valid_base_data, valid_main_data[:, -horizon:, 0], 'rmse')\n",
    "test_Y = labels_selection(test_base_data, test_main_data[:, -horizon:, 0], 'rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "92ef8a53-5fa9-42a7-bdde-03c9b4a0439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(mode):\n",
    "    start, end = mode_indx(mode)\n",
    "    # read csv files\n",
    "    slots = []\n",
    "    if use_influentional:\n",
    "        dim_range = 5\n",
    "    else:\n",
    "        dim_range = 1\n",
    "    for slot in range(start, end):\n",
    "        slot_dims = []\n",
    "        for dim in range(dim_range):\n",
    "            slot_dim_df = pd.read_csv(f'features/slot_{slot}_h{horizon}_dim{dim}.csv')\n",
    "            slot_dims.append(slot_dim_df.add_prefix(f'dim_{dim}_'))\n",
    "        slots.append(pd.concat(slot_dims, axis=1))\n",
    "    combined = pd.concat(slots).reset_index().drop(['index'] + [f'dim_{d}_unique_id' for d in range(dim_range)], axis=1).fillna(0)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "418be034-1066-4677-ae1f-5c9158e14323",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = load_features(\"train\").values\n",
    "valid_X = load_features(\"valid\").values\n",
    "test_X = load_features(\"test\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81bc2b-bbc6-451e-8bcd-4b220f180b03",
   "metadata": {},
   "source": [
    "## Selecting best forecaster using XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "02593b8b-328b-4ce4-8078-b51025561f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "7e502cb3-1d43-432c-9e33-5a57f54db244",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.XGBClassifier(eta=0.575188, max_depth=14, subsample=0.9161483,\n",
    "                        colsample_bytree = 0.7670739, objective='multi:softmax', num_class=8,\n",
    "                        random_state=0, n_estimators=100, verbosity=2, tree_method='gpu_hist', gpu_id=0,\n",
    "                        early_stopping_rounds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "f16f3b91-629b-4296-a5f8-de42f300fa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:2.11179\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-17 {color: black;}#sk-container-id-17 pre{padding: 0;}#sk-container-id-17 div.sk-toggleable {background-color: white;}#sk-container-id-17 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-17 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-17 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-17 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-17 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-17 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-17 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-17 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-17 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-17 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-17 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-17 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-17 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-17 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-17 div.sk-item {position: relative;z-index: 1;}#sk-container-id-17 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-17 div.sk-item::before, #sk-container-id-17 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-17 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-17 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-17 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-17 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-17 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-17 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-17 div.sk-label-container {text-align: center;}#sk-container-id-17 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-17 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-17\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7670739, early_stopping_rounds=1,\n",
       "              enable_categorical=False, eta=0.575188, eval_metric=None,\n",
       "              feature_types=None, gamma=None, gpu_id=0, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=14,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_class=8, num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" checked><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7670739, early_stopping_rounds=1,\n",
       "              enable_categorical=False, eta=0.575188, eval_metric=None,\n",
       "              feature_types=None, gamma=None, gpu_id=0, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=14,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_class=8, num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7670739, early_stopping_rounds=1,\n",
       "              enable_categorical=False, eta=0.575188, eval_metric=None,\n",
       "              feature_types=None, gamma=None, gpu_id=0, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=14,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              num_class=8, num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst.fit(train_X, train_Y, eval_set=[(valid_X, valid_Y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "45550c23-3575-4c45-bd91-b8c69e1b1402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_forecasts(labels, base_data):\n",
    "    forecasts = []\n",
    "    for i in range(base_data.shape[0]):\n",
    "        forecasts.append(base_data[i, labels[i], :])\n",
    "    return np.vstack(forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "b523902f-9ce6-4772-a46f-64ff3866a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = generate_forecasts(bst.predict(train_X), train_base_data)\n",
    "valid_predictions = generate_forecasts(bst.predict(valid_X), valid_base_data)\n",
    "test_predictions = generate_forecasts(bst.predict(test_X), test_base_data)\n",
    "combined_pred = np.vstack((train_predictions, valid_predictions, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "7321bd68-721b-48b3-9f00-7dac795c15de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records(combined_pred).to_csv(f'{model}-h{horizon}', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
